\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

\noindent
Neural networks often fail to generalize to sequence lengths beyond those seen during training and can degrade in accuracy or stability as depth increases. We propose to address both issues by enforcing \emph{inductive consistency}: the idea that a learned summary of the hidden state should evolve according to a simple, predictable rule across time steps or layers, in the spirit of the principle of mathematical induction (PMI). We introduce \textbf{ION} (Inductive invariant for Ordered Networks), which augments recurrent or feedforward models with a learned invariant map $P$ and transition rule $F$, and trains with a combined loss $\mathcal{L} = \mathcal{L}_{\mathrm{task}} + \lambda \mathcal{L}_{\mathrm{ind}}$, where $\mathcal{L}_{\mathrm{ind}}$ penalizes deviation from $p_{t+1} \approx F(p_t, x_t)$ (recurrent) or $p^{(l+1)} \approx F(p^{(l)})$ (layer-depth). We give a theoretical link from small invariant error to bounded task error at extrapolated length (under a Lipschitz task-head assumption). Empirically, ION is competitive on length generalization (cumulative sum, parity, Dyck-2), on LRA ListOps and LRA Image (CIFAR-10 as sequence), and maintains or matches accuracy on MNIST and CIFAR-10 at moderate depth; we report mitigations and limitations for very deep networks. Other LRA tasks, CLRS, and long-context settings remain future work.
