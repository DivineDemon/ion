\section{Conclusion}
\label{sec:conclusion}

\noindent
Neural networks often fail to generalize to longer sequence lengths and can degrade in accuracy or stability as depth increases. We proposed ION, which enforces inductive consistency by learning an invariant map $P$ and a transition rule $F$, and training with a combined loss $\mathcal{L} = \mathcal{L}_{\mathrm{task}} + \lambda \mathcal{L}_{\mathrm{ind}}$. We gave both a recurrent formulation (for sequences) and a universal layer-depth formulation (for feedforward nets). Theoretically, we showed that small invariant error implies bounded deviation of the predicted invariant trajectory (Proposition~\ref{prop:extrapolation}), and under a Lipschitz task-head assumption, bounded task error at extrapolated length (Proposition~\ref{prop:task-error}). Empirically, ION is competitive on length generalization (Tables~\ref{tab:length_gen_cumsum}--\ref{tab:length_gen_dyck2}) and maintains or matches accuracy on MNIST at moderate depth (Table~\ref{tab:depth}); we reported mitigations (warmup, gradient clipping, reduced $\lambda$ at high depth) and the depth-collapse limitation. Future work may scale ION to Long Range Arena or CLRS and explore connections to local or layer-wise learning.
