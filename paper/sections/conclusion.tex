\section{Conclusion}
\label{sec:conclusion}

\noindent
Neural networks often fail to generalize to longer sequence lengths and can degrade in accuracy or stability as depth increases. We proposed ION, which enforces inductive consistency by learning an invariant map $P$ and a transition rule $F$, and training with a combined loss $\mathcal{L} = \mathcal{L}_{\mathrm{task}} + \lambda \mathcal{L}_{\mathrm{ind}}$. We gave both a recurrent formulation (for sequences) and a universal layer-depth formulation (for feedforward nets). Theoretically, we showed that small invariant error implies bounded deviation of the predicted invariant trajectory (Proposition~\ref{prop:extrapolation}), and under a Lipschitz task-head assumption, bounded task error at extrapolated length (Proposition~\ref{prop:task-error}). Empirically, ION is competitive on length generalization (Tables~\ref{tab:length_gen_cumsum}--\ref{tab:length_gen_dyck2}), on LRA ListOps (Table~\ref{tab:lra_listops}), and maintains or matches accuracy on MNIST and CIFAR-10 at moderate depth (Tables~\ref{tab:depth} and \ref{tab:depth_cifar}); we reported mitigations (warmup, gradient clipping, reduced $\lambda$ at high depth) and the depth-collapse limitation. Future work may scale ION to additional LRA tasks or CLRS and explore connections to local or layer-wise learning.
